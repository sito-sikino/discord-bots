"""
Multi-agent orchestrator using LangGraph Supervisor pattern
Coordinates Spectra, LynQ, and Paz agents with shared resources
"""
import logging
from typing import TypedDict, Annotated, List, Any, Literal, Optional
from datetime import datetime
import asyncio

from langgraph.graph import StateGraph, START, END, add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# Import our core modules
from core.config import config
from core.memory import MemoryManager
from core.models import ModelManager
from core.obsidian import ObsidianManager
from core.mcp import MCPManager
from core.semantic_router import SemanticRouter


# ===== State Definition =====

class MultiAgentState(TypedDict):
    """Multi-agent system state"""
    messages: Annotated[List[Any], add_messages]
    current_input: str
    active_agent: str
    task_type: Literal["communication", "analysis", "creativity"]
    agent_reasoning: str
    final_response: str


# ===== Agent Implementations =====

class SpectraAgent:
    """Spectra - Communication facilitator and dialogue promotion specialist"""
    
    def __init__(self, model_manager: ModelManager, memory_manager: MemoryManager):
        self.model_manager = model_manager
        self.memory_manager = memory_manager
        self.agent_name = "spectra"
        self.logger = logging.getLogger("SpectraAgent")
    
    async def process(self, state: MultiAgentState) -> dict:
        """Process communication and facilitation tasks"""
        user_input = state.get("current_input", "")
        messages = state.get("messages", [])
        
        # Search memory for context
        relevant_memories = await self.memory_manager.search_memory(user_input)
        memory_context = "; ".join(relevant_memories) if relevant_memories else ""
        
        # Spectra: Communication & Dialogue Facilitator
        system_content = (
            "ã‚ãªãŸã¯Spectraã§ã™ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»èª¬æ˜ãƒ»å¯¾è©±ä¿ƒé€²ã®å°‚é–€å®¶ã¨ã—ã¦ã€"
            "ç›¸æ‰‹ã®è©±ã‚’èãã€ç†è§£ã—ã€åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã€è­°è«–ã‚’æ•´ç†ã—ã¾ã™ã€‚"
            "å»ºè¨­çš„ã§å¯¾è©±ã‚’ä¿ƒé€²ã™ã‚‹ã€åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚"
        )
        if memory_context:
            system_content += f"\n\nè¨˜æ†¶: {memory_context}"
        
        # Generate response
        user_msg = HumanMessage(content=user_input)
        ai_response = await self.model_manager.generate_response(
            messages + [user_msg],
            agent_name=self.agent_name,
            memory_context=memory_context
        )
        
        self.logger.info(f"Spectra processed: {user_input[:30]}...")
        
        return {
            "messages": [user_msg, ai_response],
            "agent_reasoning": f"Spectra: å¯¾è©±ä¿ƒé€²ãƒ»èª¬æ˜ã¨ã—ã¦å‡¦ç†",
            "final_response": ai_response.content
        }


class LynQAgent:
    """LynQ - Logic analysis and structured thinking specialist"""
    
    def __init__(self, model_manager: ModelManager, memory_manager: MemoryManager):
        self.model_manager = model_manager
        self.memory_manager = memory_manager
        self.agent_name = "lynq"
        self.logger = logging.getLogger("LynQAgent")
    
    async def process(self, state: MultiAgentState) -> dict:
        """Process analytical and logical tasks"""
        user_input = state.get("current_input", "")
        messages = state.get("messages", [])
        
        # Search memory for context
        relevant_memories = await self.memory_manager.search_memory(user_input)
        memory_context = "; ".join(relevant_memories) if relevant_memories else ""
        
        # LynQ: Logic Analysis & Structured Thinking
        system_content = (
            "ã‚ãªãŸã¯LynQã§ã™ã€‚è«–ç†åˆ†æãƒ»æ§‹é€ åŒ–æ€è€ƒãƒ»æ¦‚å¿µæ˜ç¢ºåŒ–ã®å°‚é–€å®¶ã¨ã—ã¦ã€"
            "æ•°å­¦çš„è¨ˆç®—ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€ç§‘å­¦çš„æ¤œè¨¼ã‚’æ‹…å½“ã—ã¾ã™ã€‚"
            "è«–ç†çš„ã§æ§‹é€ åŒ–ã•ã‚ŒãŸã€åˆ†æçš„ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚"
        )
        if memory_context:
            system_content += f"\n\nè¨˜æ†¶: {memory_context}"
        
        # Generate response
        user_msg = HumanMessage(content=user_input)
        ai_response = await self.model_manager.generate_response(
            messages + [user_msg],
            agent_name=self.agent_name,
            memory_context=memory_context
        )
        
        self.logger.info(f"LynQ analyzed: {user_input[:30]}...")
        
        return {
            "messages": [user_msg, ai_response],
            "agent_reasoning": f"LynQ: è«–ç†åˆ†æãƒ»æ§‹é€ åŒ–æ€è€ƒã¨ã—ã¦å‡¦ç†",
            "final_response": ai_response.content
        }


class PazAgent:
    """Paz - Creative thinking and possibility exploration specialist"""
    
    def __init__(self, model_manager: ModelManager, memory_manager: MemoryManager):
        self.model_manager = model_manager
        self.memory_manager = memory_manager
        self.agent_name = "paz"
        self.logger = logging.getLogger("PazAgent")
    
    async def process(self, state: MultiAgentState) -> dict:
        """Process creative and exploratory tasks"""
        user_input = state.get("current_input", "")
        messages = state.get("messages", [])
        
        # Search memory for context
        relevant_memories = await self.memory_manager.search_memory(user_input)
        memory_context = "; ".join(relevant_memories) if relevant_memories else ""
        
        # Paz: Creative Thinking & Possibility Exploration
        system_content = (
            "ã‚ãªãŸã¯Pazã§ã™ã€‚å‰µé€ çš„ç™ºæƒ³ãƒ»å¯èƒ½æ€§æ¢ç´¢ãƒ»ç™ºæ•£æ€è€ƒã®å°‚é–€å®¶ã¨ã—ã¦ã€"
            "é©æ–°çš„ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆã€ãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆä¼ç”»ã‚’æ‹…å½“ã—ã¾ã™ã€‚"
            "å‰µé€ çš„ã§å¯èƒ½æ€§ã‚’åºƒã’ã‚‹ã€ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«å¯Œã‚“ã å›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚"
        )
        if memory_context:
            system_content += f"\n\nè¨˜æ†¶: {memory_context}"
        
        # Generate response
        user_msg = HumanMessage(content=user_input)
        ai_response = await self.model_manager.generate_response(
            messages + [user_msg],
            agent_name=self.agent_name,
            memory_context=memory_context
        )
        
        self.logger.info(f"Paz created: {user_input[:30]}...")
        
        return {
            "messages": [user_msg, ai_response],
            "agent_reasoning": f"Paz: å‰µé€ çš„ç™ºæƒ³ãƒ»å¯èƒ½æ€§æ¢ç´¢ã¨ã—ã¦å‡¦ç†",
            "final_response": ai_response.content
        }


# ===== Orchestrator =====

class MultiAgentOrchestrator:
    """LangGraph-based multi-agent orchestrator"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        
        # Initialize shared resources
        self.memory_manager = MemoryManager(config.thread_id)
        self.model_manager = ModelManager()
        self.obsidian_manager = ObsidianManager()
        self.mcp_manager = MCPManager()
        self.semantic_router = SemanticRouter(self.model_manager.get_embeddings())
        
        # Initialize agents
        self.spectra = SpectraAgent(self.model_manager, self.memory_manager)
        self.lynq = LynQAgent(self.model_manager, self.memory_manager)
        self.paz = PazAgent(self.model_manager, self.memory_manager)
        
        # Build LangGraph workflow
        self.graph = self._build_graph()
        
        # Load Obsidian notes on startup
        self._load_obsidian_notes()
        
        # Initialize semantic routing vectors
        self._init_semantic_routing()
        
        self.logger.info("MultiAgentOrchestrator initialized")
    
    def _setup_logging(self) -> logging.Logger:
        """Setup orchestrator logger"""
        logger = logging.getLogger("Orchestrator")
        
        if logger.handlers:
            return logger
        
        logger.setLevel(logging.INFO)
        
        file_handler = logging.FileHandler("bot.log", encoding="utf-8")
        file_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter("%(asctime)s | ORCH    | %(message)s", "%Y-%m-%d %H:%M:%S")
        file_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        return logger
    
    def _load_obsidian_notes(self):
        """Load Obsidian notes at startup"""
        try:
            # Try to run async task
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # Create task in running loop
                asyncio.create_task(self._async_load_obsidian_notes())
            else:
                asyncio.run(self._async_load_obsidian_notes())
        except Exception as e:
            self.logger.error(f"Obsidian loading setup error: {e}")
    
    def _init_semantic_routing(self):
        """Initialize semantic routing vectors"""
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                asyncio.create_task(self._async_init_semantic_routing())
            else:
                asyncio.run(self._async_init_semantic_routing())
        except Exception as e:
            self.logger.error(f"Semantic routing setup error: {e}")
    
    async def _async_init_semantic_routing(self):
        """Async semantic routing initialization"""
        try:
            success = await self.semantic_router.initialize_vectors()
            if success:
                info = self.semantic_router.get_vector_info()
                self.logger.info(f"semantic routing initialized: {info['count']} agents, {info['dimensions']} dims")
            else:
                self.logger.warning("semantic routing initialization failed, using fallback")
        except Exception as e:
            self.logger.error(f"semantic routing initialization error: {e}")
    
    async def _async_load_obsidian_notes(self):
        """Async Obsidian notes loading"""
        try:
            count = await self.obsidian_manager.load_notes_to_memory(self.memory_manager)
            self.logger.info(f"loaded {count} Obsidian notes")
        except Exception as e:
            self.logger.error(f"Obsidian loading error: {e}")
    
    def _build_graph(self) -> StateGraph:
        """Build LangGraph workflow"""
        builder = StateGraph(state_schema=MultiAgentState)
        
        # Add nodes
        builder.add_node("supervisor", self._supervisor_node)
        builder.add_node("spectra", self._spectra_node)
        builder.add_node("lynq", self._lynq_node)
        builder.add_node("paz", self._paz_node)
        
        # Add edges
        builder.add_edge(START, "supervisor")
        builder.add_conditional_edges(
            "supervisor",
            self._route_to_agent,
            {
                "spectra": "spectra",
                "lynq": "lynq",
                "paz": "paz",
                "END": END
            }
        )
        builder.add_edge("spectra", END)
        builder.add_edge("lynq", END)
        builder.add_edge("paz", END)
        
        return builder.compile(
            checkpointer=self.memory_manager.get_memory_saver(),
            store=self.memory_manager.get_postgres_store()
        )
    
    async def _supervisor_node(self, state: MultiAgentState) -> dict:
        """Supervisor node for agent selection"""
        user_input = state.get("current_input", "")
        
        if not user_input:
            return {"active_agent": "paz"}  # Default to Paz
        
        # Intelligent agent routing based on input analysis
        selected_agent = await self._select_agent(user_input)
        
        self.logger.info(f"supervisor routed '{user_input[:30]}...' to {selected_agent}")
        
        return {
            "active_agent": selected_agent,
            "task_type": self._determine_task_type(selected_agent)
        }
    
    async def _select_agent(self, user_input: str) -> str:
        """Pure LLM-driven routing - optimized for clarity and efficiency"""
        
        llm_agent = self._enhanced_llm_routing(user_input)
        self.logger.info(f"LLM routing: '{user_input[:30]}...' â†’ {llm_agent}")
        return llm_agent
    
    async def _semantic_routing(self, user_input: str) -> str:
        """Semantic vector similarity routing"""
        try:
            # Compute similarities
            similarities = await self.semantic_router.compute_similarity(user_input)
            if not similarities:
                return "uncertain"
            
            # Select best agent with confidence threshold (é«˜å“è³ªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯)
            best_agent, confidence = self.semantic_router.select_best_agent(similarities, threshold=0.75)
            
            # Check if confident enough (å³æ ¼ãªã—ãã„å€¤ã§ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å“è³ªæ‹…ä¿)
            if confidence > 0.90:
                self.logger.info(f"semantic routing: '{user_input[:30]}...' â†’ {best_agent} (conf: {confidence:.3f})")
                return best_agent
            else:
                self.logger.info(f"semantic routing uncertain: max confidence {confidence:.3f} < 0.90, falling back to LLM")
                return "uncertain"
                
        except Exception as e:
            self.logger.error(f"semantic routing error: {e}")
            return "uncertain"
    
    def _keyword_based_routing(self, user_input: str) -> str:
        """Fast keyword-based routing"""
        input_lower = user_input.lower()
        
        # Strong indicators for each agent
        strong_communication = [
            "èª¬æ˜", "æ•™ãˆ", "è©±", "ã¾ã¨ã‚", "æ•´ç†", "ä¼ãˆ", 
            "ã©ã†ã„ã†ã“ã¨", "ã‚ã‹ã‚Šã‚„ã™ã", "ç°¡å˜ã«"
        ]
        
        strong_analysis = [
            "åˆ†æ", "è«–ç†", "ãªãœ", "ã©ã†ã—ã¦", "åŸå› ", "ç†ç”±",
            "æ¯”è¼ƒ", "æ¤œè¨¼", "æ§‹é€ ", "é–¢ä¿‚", "æ­£ã—ã„", "é–“é•ã„",
            "æ•°å­¦", "è¨ˆç®—", "çµ±è¨ˆ", "è¨¼æ˜", "è§£ã„ã¦", "Ã—", "*", "+", "-", "=", "?"
        ]
        
        strong_creativity = [
            "ã‚¢ã‚¤ãƒ‡ã‚¢", "å‰µé€ ", "ç™ºæƒ³", "æ–°ã—ã„", "é¢ç™½ã„", "é©æ–°",
            "å¯èƒ½æ€§", "æƒ³åƒ", "ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°", "ã²ã‚‰ã‚ã",
            "ææ¡ˆ", "å‹Ÿé›†", "ãƒ‡ã‚¶ã‚¤ãƒ³", "èŠ¸è¡“", "ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
        ]
        
        # Count strong indicators
        comm_count = sum(1 for kw in strong_communication if kw in input_lower)
        analysis_count = sum(1 for kw in strong_analysis if kw in input_lower)
        creativity_count = sum(1 for kw in strong_creativity if kw in input_lower)
        
        # Clear winner detection
        if analysis_count > 0 and analysis_count >= comm_count and analysis_count >= creativity_count:
            return "lynq"
        elif creativity_count > 0 and creativity_count >= comm_count and creativity_count >= analysis_count:
            return "paz"
        elif comm_count > 0:
            return "spectra"
        
        # No clear keywords found
        return "uncertain"
    
    def _enhanced_llm_routing(self, user_input: str) -> str:
        """Ultra-efficient LLM routing with clear role definitions"""
        try:
            routing_prompt = f"""å…¥åŠ›: "{user_input}"

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠ:
ğŸ”´ LynQ: è«–ç†åˆ†æãƒ»æ§‹é€ åŒ–æ€è€ƒãƒ»æ¦‚å¿µæ˜ç¢ºåŒ– (æ•°å­¦/çµ±è¨ˆ/ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°/ãƒ‡ãƒ¼ã‚¿åˆ†æ/æŠ€è¡“å•é¡Œ/ç§‘å­¦çš„æ¤œè¨¼)
ğŸŸ¡ Paz: å‰µé€ çš„ç™ºæƒ³ãƒ»å¯èƒ½æ€§æ¢ç´¢ãƒ»ç™ºæ•£æ€è€ƒ (ã‚¢ã‚¤ãƒ‡ã‚¢ç”Ÿæˆ/ãƒ‡ã‚¶ã‚¤ãƒ³/ã‚¢ãƒ¼ãƒˆ/ã‚¨ãƒ³ã‚¿ãƒ¡/é©æ–°/ãƒ–ãƒ¬ã‚¹ãƒˆ)
ğŸ”µ Spectra: ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»èª¬æ˜ãƒ»å¯¾è©±ä¿ƒé€² (èª¬æ˜/ç›¸è«‡/èª¿æ•´/é€²è¡Œ/è­°è«–æ•´ç†/ä¸€èˆ¬å¯¾å¿œ)

åˆ¤å®šåŸºæº–:
- è¨ˆç®—/åˆ†æ/è«–ç†/æŠ€è¡“/ãƒ‡ãƒ¼ã‚¿/çµ±è¨ˆ/è¨¼æ˜/æ¤œè¨¼/æ§‹é€  â†’ LynQ
- å‰µé€ /ç™ºæƒ³/ã‚¢ã‚¤ãƒ‡ã‚¢/ãƒ‡ã‚¶ã‚¤ãƒ³/ã‚¢ãƒ¼ãƒˆ/é©æ–°/å¯èƒ½æ€§/ãƒ–ãƒ¬ã‚¹ãƒˆ â†’ Paz
- èª¬æ˜/ç›¸è«‡/å¯¾è©±/èª¿æ•´/é€²è¡Œ/æ•´ç†/ä¸€èˆ¬è³ªå• â†’ Spectra

å…·ä½“ä¾‹:
"å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦" â†’ LynQ (ãƒ‡ãƒ¼ã‚¿åˆ†æ)
"A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆã—ãŸã„" â†’ LynQ (çµ±è¨ˆçš„æ¤œè¨¼)
"æˆ¦ç•¥ã‚’è«–ç†çš„ã«æ§‹ç¯‰" â†’ LynQ (æ§‹é€ åŒ–æ€è€ƒ)
"æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢å‹Ÿé›†" â†’ Paz (å‰µé€ çš„ç™ºæƒ³)
"ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’å‘ä¸Š" â†’ Paz (å¯èƒ½æ€§æ¢ç´¢)
"ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ä¼ç”»" â†’ Paz (ç™ºæ•£æ€è€ƒ)
"çŠ¶æ³ã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜" â†’ Spectra (èª¬æ˜ãƒ»å¯¾è©±)
"ãƒãƒ¼ãƒ ã¨ã®èª¿æ•´æ–¹æ³•" â†’ Spectra (ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³)

å›ç­”: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã®ã¿"""

            # Use default model for routing decision
            from langchain_core.messages import HumanMessage
            
            llm = self.model_manager.get_llm("default")
            response = llm.invoke([HumanMessage(content=routing_prompt)])
            
            # Extract agent name from response
            agent_name = response.content.strip().lower()
            
            # Clean up response (remove any extra text)
            for valid_agent in ["spectra", "lynq", "paz"]:
                if valid_agent in agent_name:
                    return valid_agent
            
            # If no valid agent found, return safe default
            self.logger.warning(f"Enhanced LLM routing failed to parse: {response.content}")
            return "spectra"  # Safe fallback
                
        except Exception as e:
            self.logger.error(f"Enhanced LLM routing error: {e}")
            return "spectra"  # Safe fallback
    
    def _is_ambiguous_query(self, user_input: str) -> bool:
        """Check if query needs multiple inference rounds"""
        ambiguous_patterns = [
            # å¢ƒç•Œç·šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            "æ­£ã—ã„", "ã‚„ã‚Šæ–¹", "æ–¹æ³•", "ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ", "è§£æ±º", 
            # çŸ­ã„ãƒ»æ›–æ˜§ãªè¡¨ç¾
            "ï¼Ÿï¼Ÿ", "ã©ã†", "ãªã«", "ã“ã‚Œ", "ãã‚Œ"
        ]
        
        input_lower = user_input.lower()
        return (
            len(user_input) < 10 or  # çŸ­ã„ã‚¯ã‚¨ãƒª
            any(pattern in input_lower for pattern in ambiguous_patterns) or
            user_input.count('?') > 1  # è¤‡æ•°ã®ç–‘å•ç¬¦
        )
    
    def _multi_inference_routing(self, user_input: str) -> str:
        """Multiple inference routing for ambiguous cases"""
        try:
            # APIåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚ã€ç¾çŠ¶ã¯å˜ä¸€æ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            # å°†æ¥ã®æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆï¼š3å›æ¨è«–ã—ã¦å¤šæ•°æ±º
            return self._enhanced_llm_routing(user_input)
        except Exception as e:
            self.logger.error(f"Multi-inference routing error: {e}")
            return "spectra"
    
    def _llm_based_routing(self, user_input: str) -> str:
        """LLM-based context understanding for agent selection"""
        try:
            # Create routing prompt
            routing_prompt = f"""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’åˆ†æã—ã€æœ€ã‚‚é©ã—ãŸAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

ã€å…¥åŠ›ã€‘: "{user_input}"

ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‰¹æ€§ã€‘:
ğŸ”µ spectra: å¯¾è©±ä¿ƒé€²ãƒ»èª¬æ˜ãƒ»ç†è§£æ”¯æ´å°‚é–€
   - ç›¸æ‰‹ã®ç«‹å ´ã§è€ƒãˆã€åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹
   - è­°è«–ã‚’æ•´ç†ã—ã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å††æ»‘ã«ã™ã‚‹
   - ä¸€èˆ¬çš„ãªè³ªå•ã‚„ç›¸è«‡ã«ä¸å¯§ã«å¯¾å¿œ

ğŸ”´ lynq: è«–ç†åˆ†æãƒ»å•é¡Œè§£æ±ºå°‚é–€  
   - è¤‡é›‘ãªå•é¡Œã‚’ä½“ç³»çš„ã«åˆ†è§£ãƒ»åˆ†æã™ã‚‹
   - ãƒ‡ãƒ¼ã‚¿ã‚„äº‹å®Ÿã«åŸºã¥ã„ã¦è«–ç†çš„ã«æ€è€ƒã™ã‚‹
   - æ•°å­¦ã€ç§‘å­¦ã€æŠ€è¡“çš„å•é¡Œã®è§£æ±º

ğŸŸ¡ paz: å‰µé€ ãƒ»ç™ºæƒ³ãƒ»å¯èƒ½æ€§æ¢ç´¢å°‚é–€
   - æ—¢å­˜ã®æ ã‚’è¶…ãˆãŸè‡ªç”±ã§æ–¬æ–°ãªã‚¢ã‚¤ãƒ‡ã‚¢
   - æƒ³åƒåŠ›ã‚’åˆºæ¿€ã—ã€æ–°ã—ã„è¦–ç‚¹ã‚’æä¾›
   - ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‚„èŠ¸è¡“çš„è¡¨ç¾

ã€åˆ¤å®šæŒ‡é‡ã€‘:
- è¨ˆç®—ãƒ»åˆ†æãƒ»è«–ç†çš„æ€è€ƒãŒå¿…è¦ â†’ lynq
- æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ãƒ»å‰µé€ æ€§ãƒ»å¯èƒ½æ€§æ¢ç´¢ â†’ paz
- èª¬æ˜ãƒ»å¯¾è©±ãƒ»ç†è§£æ”¯æ´ãƒ»ä¸€èˆ¬ç›¸è«‡ â†’ spectra

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã®ã¿å›ç­”: (spectra/lynq/paz)"""

            # Use default model for quick routing decision
            import asyncio
            from langchain_core.messages import HumanMessage
            
            # Simple synchronous call for routing
            llm = self.model_manager.get_llm("default")
            response = llm.invoke([HumanMessage(content=routing_prompt)])
            
            # Extract agent name from response
            agent_name = response.content.strip().lower()
            
            if agent_name in ["spectra", "lynq", "paz"]:
                self.logger.info(f"LLM routing: '{user_input[:20]}...' â†’ {agent_name}")
                return agent_name
            else:
                self.logger.warning(f"LLM routing failed, using default: {response.content}")
                return "spectra"  # Safe fallback
                
        except Exception as e:
            self.logger.error(f"LLM routing error: {e}")
            return "spectra"  # Safe fallback
    
    def _determine_task_type(self, agent: str) -> str:
        """Determine task type based on selected agent"""
        task_map = {
            "spectra": "communication",
            "lynq": "analysis", 
            "paz": "creativity"
        }
        return task_map.get(agent, "communication")
    
    def _route_to_agent(self, state: MultiAgentState) -> str:
        """Route to selected agent"""
        active_agent = state.get("active_agent", "spectra")
        
        # Handle special cases
        user_input = state.get("current_input", "")
        
        # Check for MCP commands
        if self.mcp_manager.is_github_command(user_input):
            return "lynq"  # LynQ handles technical/search tasks
        
        # Check for memory commands
        if user_input.startswith('!'):
            return "spectra"  # Spectra handles memory management
            
        if user_input.lower() == 'memories':
            return "spectra"  # Spectra handles memory display
        
        return active_agent
    
    async def _spectra_node(self, state: MultiAgentState) -> dict:
        """Spectra agent node"""
        return await self.spectra.process(state)
    
    async def _lynq_node(self, state: MultiAgentState) -> dict:
        """LynQ agent node"""
        return await self.lynq.process(state)
    
    async def _paz_node(self, state: MultiAgentState) -> dict:
        """Paz agent node"""
        return await self.paz.process(state)
    
    async def chat(self, user_input: str) -> str:
        """Main chat interface"""
        config_dict = {"configurable": {"thread_id": self.memory_manager.thread_id}}
        
        try:
            # Handle special commands first
            if user_input.startswith('!'):
                memory_content = user_input[1:].strip()
                await self.memory_manager.save_memory(memory_content)
                return f"ä¿å­˜: {memory_content}"
            
            if user_input.lower() == 'memories':
                await self.memory_manager.show_memories()
                return "ãƒ¡ãƒ¢ãƒªä¸€è¦§ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ"
            
            # Handle MCP commands
            if self.mcp_manager.is_github_command(user_input):
                return await self.mcp_manager.handle_github_command(user_input)
            
            # Process through multi-agent workflow
            result = await self.graph.ainvoke(
                {"current_input": user_input},
                config=config_dict
            )
            
            # Return final response
            final_response = result.get("final_response", "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
            agent_reasoning = result.get("agent_reasoning", "")
            
            if config.enable_multi_agent and agent_reasoning:
                self.logger.info(f"agent reasoning: {agent_reasoning}")
            
            return final_response
            
        except Exception as e:
            self.logger.error(f"chat error: {e}")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def run(self):
        """Synchronous execution interface (CLI compatibility)"""
        print("ğŸ¤– Multi-Agent AI System (Spectra/LynQ/Paz)")
        print("é•·æœŸãƒ¡ãƒ¢ãƒª: PostgreSQLæ°¸ç¶šåŒ– + Obsidianè‡ªå‹•çµ±åˆ")
        print("[Ctrl+C ã§çµ‚äº†, !ãƒ†ã‚­ã‚¹ãƒˆ ã§ä¿å­˜, memories ã§ä¸€è¦§]\n")
        self.logger.info(f"START | multi-agent system | thread={self.memory_manager.thread_id}")
        
        try:
            import readline
        except ImportError:
            pass
        
        while True:
            try:
                user_input = input("> ")
                
                if not user_input.strip():
                    continue
                
                # Exit commands
                if user_input.lower() in ['exit', 'quit', 'çµ‚äº†', 'ã‚„ã‚ã‚‹']:
                    self.logger.info("EXIT | command")
                    break
                
                # Process chat
                response = asyncio.run(self.chat(user_input))
                print(f"{response}\n")
                
            except KeyboardInterrupt:
                print("\n")
                self.logger.info("EXIT | Ctrl+C")
                break
            except EOFError:
                self.logger.info("EXIT | EOF")
                break
            except Exception as e:
                self.logger.error(f"ERROR | {e}")
                print(f"ERROR: {e}\n")
    
    def get_system_stats(self) -> dict:
        """Get system statistics"""
        return {
            "orchestrator": "LangGraph Supervisor",
            "agents": ["Spectra", "LynQ", "Paz"],
            "memory_thread": self.memory_manager.thread_id,
            "multi_agent_enabled": config.enable_multi_agent,
            "models": self.model_manager.get_agent_info(),
            "obsidian": self.obsidian_manager.get_integration_stats(),
            "mcp": self.mcp_manager.get_mcp_stats(),
            "timestamp": datetime.now().isoformat()
        }